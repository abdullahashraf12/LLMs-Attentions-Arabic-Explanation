<!DOCTYPE html>
<html lang="ar" dir="rtl">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ุงูุชุตููู ุงูุนููู ุงูุดุงูู ูุขููุงุช ุงูุงูุชุจุงู (27 ููุน)</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.2/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet">
    <link href="styles.css" rel="stylesheet">

</head>

<body>
    <div class="container" style="max-width: 100%;">
        <div class="fade-in">
            <div class="header-section">
                <h1><span class="emoji-icon">๐ง</span> ุงูุชุตููู ุงูุนููู ุงูุดุงูู ูุขููุงุช ุงูุงูุชุจุงู</h1>
                <p class="subtitle">
                    ุฏููู ุดุงูู ูู 27 ููุนูุง ูู ุขููุงุช ุงูุงูุชุจุงู ูู ุงูุชุนูู ุงูุนูููุ ููุณู ุฅูู ุขููุงุช ุฑูุงุถูุฉ ุฃุณุงุณูุฉุ ุชุญุณููุงุช ุชูููุฐูุฉุ ููุนูุงุฑูุงุช ูุชูุฏูุฉ
                </p>
            </div>


            <div class="stats-bar">
                <div class="stat-item">
                    <div class="stat-value" id="totalMechanisms">27</div>
                    <div class="stat-label">ุขููุฉ ุฑูุงุถูุฉ</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value" id="totalOptimizations">5</div>
                    <div class="stat-label">ุชุญุณูู ุชูููุฐู</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value" id="totalFound">32</div>
                    <div class="stat-label">ุฅุฌูุงูู ุงูุฃููุงุน</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value">2025</div>
                    <div class="stat-label">ุขุฎุฑ ุชุญุฏูุซ</div>
                </div>
            </div>

            <div class="search-container">
                <input type="text" class="form-control search-input" id="searchInput" placeholder="ุงุจุญุซ ูู ุขููุงุช ุงูู Attention... (ูุซุงู: Sparse, Linear, Flash, PyTorch)">
                <i class="fas fa-search search-icon"></i>
                <span class="clear-search" id="clearSearch" title="ูุณุญ ุงูุจุญุซ">
                    <i class="fas fa-times-circle"></i>
                </span>
            </div>

            <div class="filter-buttons">
                <button class="filter-btn active" data-filter="all">ุงููู (27)</button>
                <button class="filter-btn" data-filter="mechanisms">ุขููุงุช ุงูุงูุชุจุงู (22)</button>
                <button class="filter-btn" data-filter="optimizations">ุชุญุณููุงุช ุชูููุฐูุฉ (5)</button>
                <button class="filter-btn" data-filter="sparse">Sparse Attention (9)</button>
                <button class="filter-btn" data-filter="basic">ุฃุณุงุณู (5)</button>
                <button class="filter-btn" data-filter="pytorch">PyTorch (3)</button>
            </div>
            <!-- ุฌุฏูู 1: ุขููุงุช ุงูุงูุชุจุงู ุงูุฑูุงุถูุฉ -->


            <div class="container___main">
                <div class="container___main_left">
                    <h2 class="section-title" style="display: flex; justify-content: space-between; align-items: center; margin: 0; padding: 0;">
                        <span style="display: flex; align-items: center;">
            <span class="emoji-icon">๐งฎ</span> ุขููุงุช ุงูุงูุชุจุงู (ุงูุตูุบ ุงูุฑูุงุถูุฉ - 22 ููุน)
                        </span>
                        <a href="#" id="openPopup" class="popup-link" style="white-space: nowrap; margin-left: 10px;">
            ุชูุฏุฑ ุชุจุต ุนูู ุงู architecture ู GPT 2 ุจูุนุชุจุฑ ุฃุจุณุท ูุซุงู ุนููู ุนูู ุงู <span style="color: red;">Self Attention + Multi Head Attention</span>
        </a>
                    </h2>
                </div>




            </div>

            <div id="popup" class="popup-overlay">
                <div class="popup-container">
                    <iframe id="popupFrame" class="popup-iframe"></iframe>
                    <button id="closePopup" class="popup-close-btn">
            <i class="fas fa-times"></i>
        </button>
                </div>
            </div>



            <div class="table-container">
                <table class="table table-hover" id="mechanismsTable">
                    <thead>
                        <tr>
                            <th>#</th>
                            <th>ุงูุขููุฉ</th>
                            <th>ุงููุตู ุงูุนููู</th>
                            <th>ููู ุงู RNN/LSTM ุจุชุนุงูู</th>
                            <th>ุงูุญู ุงูููุตู</th>
                            <th>ุงููุดููุฉ ุงูุชู ุชุญููุง</th>
                            <th>ูุฑุงุฌุน / ุฃูุซูุฉ</th>
                        </tr>
                    </thead>
                    <tbody id="mechanismsBody">
                        <!-- ุณูุชู ููุคู ุจูุงุณุทุฉ JavaScript -->
                    </tbody>
                </table>
            </div>

            <!-- ุฌุฏูู 2: ุงูุชุญุณููุงุช ุงูุชูููุฐูุฉ ูุงููุนูุงุฑูุงุช -->
            <h2 class="section-title mt-5"><span class="emoji-icon">โ๏ธ</span> ุงูุชุญุณููุงุช ุงูุชูููุฐูุฉ ูุงููุนูุงุฑูุงุช ุงููุฑุชุจุทุฉ (5 ุฃููุงุน)</h2>

            <div class="table-container">
                <table class="table table-hover" id="optimizationsTable">
                    <thead>
                        <tr>
                            <th>#</th>
                            <th>ุงูุนูุตุฑ</th>
                            <th>ุงูุชุตููู ุงูุตุญูุญ</th>
                            <th>ุงููุธููุฉ / ุงูุชุทุจูู</th>
                            <th>ูุฑุงุฌุน / ุฃูุซูุฉ</th>
                        </tr>
                    </thead>
                    <tbody id="optimizationsBody">
                        <!-- ุณูุชู ููุคู ุจูุงุณุทุฉ JavaScript -->
                    </tbody>
                </table>
            </div>

            <div class="no-results" id="noResults">
                <i class="fas fa-search" style="font-size: 3rem; margin-bottom: 15px; display: block; color: #aaa;"></i>
                <p>ูู ูุชู ุงูุนุซูุฑ ุนูู ูุชุงุฆุฌ ูุทุงุจูุฉ ููุจุญุซ</p>
                <button class="btn btn-outline-primary mt-3" id="resetSearch">ุฅุนุงุฏุฉ ุชุนููู ุงูุจุญุซ ูุงูุชุตููุฉ</button>
            </div>

            <div class="note-box">
                <h3><i class="fas fa-lightbulb"></i> ููุงุญุธุฉ ุนูููุฉ ูููุฉ</h3>
                <ul>
                    <li>ุงูุฌุฏูู ุงูุฃูู ูุญุชูู ุนูู <strong>22 ุขููุฉ ุงูุชุจุงู ุฑูุงุถูุฉ ุญููููุฉ</strong> (Attention Formulations)</li>
                    <li>ุงูุฌุฏูู ุงูุซุงูู ูุถู <strong>5 ุชุญุณููุงุช ุชูููุฐูุฉ ููุนูุงุฑูุงุช</strong> ุชุนุชูุฏ ุนูู ุงูุงูุชุจุงู ููููุง ููุณุช ุตูุบ ุงูุชุจุงู ุจุญุฏ ุฐุงุชูุง</li>
                    <li>ุชุตููู <strong>Sparse Attention</strong> ูุดูู: Local, Strided, Global, Random, Block, Fixed Patterns, Top-K, LSH, Clustering (9 ุฃููุงุน)</li>
                    <li>ุงูุชุตููู <strong>PyTorch</strong> ูุดูู ุชูููุฐุงุช ุฌุงูุฒุฉ ูู ุฅุทุงุฑ ุงูุนูู PyTorch ูููุณุช ุขููุงุช ุฑูุงุถูุฉ ุฌุฏูุฏุฉ</li>
                    <li>ุฌููุน ุงูุฃููุงุน ุชูุฏู ูุชุญุณูู ุงูููุงุกุฉ ุงูุญุณุงุจูุฉ ุฃู ุงูุฏูุฉ ุฃู ุฅููุงููุฉ ุงูุชูุณูุฑ ูู ููุงุฐุฌ ุงูุงูุชุจุงู</li>
                </ul>
            </div>
        </div>

        <div class="footer">
            <p>ุชู ุชุตููู ูุฐุง ุงูุชุตููู ูููุทูุฑูู ูุงูุจุงุญุซูู ูู ูุฌุงู ุงูุชุนูู ุงูุนููู ูุงูุฐูุงุก ุงูุงุตุทูุงุนู</p>
            <p>ูููู ุงุณุชุฎุฏุงู ูุญุชูู ูุฐุง ุงูุฌุฏูู ุจุญุฑูุฉ ูุน ุงูุฅุดุงุฑุฉ ุฅูู ุงููุตุฏุฑ</p>
            <p>ุขุฎุฑ ุชุญุฏูุซ: ุฏูุณูุจุฑ 2025 | <a href="#" id="copyLink">ูุณุฎ ุงูุฑุงุจุท ูุน ุญุงูุฉ ุงูุจุญุซ</a></p>
        </div>
    </div>

    <div class="copy-notification" id="copyNotification">
        <i class="fas fa-check-circle"></i> ุชู ูุณุฎ ุงูุฑุงุจุท ุจูุฌุงุญ
    </div>

    <script>
        const attentionMechanisms = [{
                "id": 1,
                "type": "Basic Attention",
                "description": "ุงูุงูุชุจุงู ุงูุฃุณุงุณู - ุทุฑููุฉ ุชุญุณูู ุนูู RNN/LSTM ุงูุชูููุฏู ูุฑุจุท ูู ูููุฉ ุจูู ุงููููุงุช ุงูุฃุฎุฑู ูู ุงูุชุณูุณู.",
                "rnnProblem": "RNN/LSTM ุจุชุนุงูุฌ ุงููููุงุช ูุงุญุฏุฉ ูุงุญุฏุฉุ ูููุง ููุตู ูุขุฎุฑ ุงูุฌููุฉ ุจููุณู ุฃูู ุงูุฌููุฉ ุจุณุจุจ ูุดููุฉ vanishing gradientุ ููุง ุจููุฏุฑุด ูุฑุจุท ูููุงุช ุจุนูุฏุฉ ุจุณูููุฉ.",
                "detailedSolution": "Basic Attention ุจูุถูู ุขููุฉ ุชุฎูู ูู ุฎุทูุฉ ูู ุงููdecoder ุชูุฏุฑ 'ุชุดูู' ูู hidden states ูููencoderุ ุจุฏู ูุง ุชุนุชูุฏ ุจุณ ุนูู ุขุฎุฑ hidden state. ุงูุนูููุฉ ุชุนุชูุฏ ุนูู ุซูุงุซ ููููุงุช ุฑุฆูุณูุฉ: Q (Query) ูู ุงููdecoderุ K (Key) ูV (Value) ูู ุงููencoder. ุจูุชู ุญุณุงุจ ุชุดุงุจู ูู Q ูุน ูู K ููุญุตูู ุนูู weightsุ ูุงููweights ุฏู ุจุชุชุทุจู ุนูู ุงููV ูุชูููู context vector ููู ุฎุทูุฉ. ุฏู ุจูุฎูู ุงููููุฐุฌ ูุฑุจุท ูููุงุช ุจุนูุฏุฉ ุจุณูููุฉ ููููู ูุดููุฉ ูุณูุงู ุงููุนูููุงุช ุงููุฏููุฉ. โ๏ธ ููุญูุธุฉ: ุงูุชุนููุฏ ุนุงุฏุฉ O(n*m) ุญูุซ n ุทูู ุงููdecoder ู m ุทูู ุงููencoderุ ูุด ููุง ูู self-attention ุฏุงุฎู ููุณ ุงูุชุณูุณู.",
                "problem": "ูุญุชุงุฌูู ุฑุจุท ูููุงุช ุจุนูุฏุฉ ุนู ุจุนุถ ูู ุงูุชุณูุณูุ ุฎุตูุตูุง ูู ุงูุชุฑุฌูุฉ ุฃู ููุก ุงููุตูุต ุงูุทูููุฉ.",
                "reference": "ุงูุชุทุจููุงุช ุงูุฃุณุงุณูุฉ ููุงูุชุจุงู: machine translation, seq2seq tasks, speech recognition.",
                "category": ["mechanisms", "basic"],
                "classification": "mechanism",
                "classificationType": "basic"
            }, {
                "id": 2,
                "type": "Self-Attention",
                "description": "Q, K, V ูุฃุฎูุฐุฉ ูู ููุณ ุงูุชุณูุณู (ููุณ ุงููุตุฏุฑ)ุ ุชุชูุญ ููู ูููุฉ ุฃู ุชุฑู ูู ุงููููุงุช ุงูุฃุฎุฑู ุฏุงุฎู ููุณ ุงูุชุณูุณู ูู ููุณ ุงูููุช.",
                "rnnProblem": "RNN/LSTM ุจุชุนุงูุฌ ุงููููุงุช sequentialุ ูุจุชุงุฎุฏ ููุช ุทููู ููุด ุจุชูุฏุฑ ุชุนูู parallel processingุ ูููุงู ุตุนุจ ุนูููุง ุชุชุนุงูู ูุน ุชุณูุณูุงุช ุทูููุฉ ุจุณุจุจ vanishing/exploding gradient.",
                "detailedSolution": "Self-Attention ุจุชุญุณุจ ุนูุงูุฉ ูู ูููุฉ ูุน ูู ุงููููุงุช ุงูุฃุฎุฑู ูู ููุณ ุงูุชุณูุณู ุจุงุณุชุฎุฏุงู Q, K, V ูู ููุณ ุงููุตุฏุฑ. ุงูุนูููุฉ ุงูุฃุณุงุณูุฉ ุชูุนุฑู ุจูScaled Dot-Product Attention: <span style='color:green; display: block; direction: ltr; text-align: left;'>Attention(Q,K,V) = softmax(QK<sup>T</sup>/โd<sub>k</sub>)V</span>. ููุง <span style='color:green; display: inline-block; direction: ltr;'>โd<sub>k</sub></span> ุจูุณุชุฎุฏู ูุชูููู ุงุญุชูุงู gradient explosion. ุงูููุฒุฉ ุงูุฑุฆูุณูุฉ: ูู ุงููููุงุช ุจุชูุนุงููุฌ **ูุชูุงุฒููุง (parallel)**ุ ูุจุชููู ุงูููุงุฐุฌ ูุงุฏุฑุฉ ุนูู ุงูุฑุจุท ุจูู ุฃู ูููุงุช ูููุง ูุงูุช ุงููุณุงูุฉ ุจูููู ุฏุงุฎู ุงูุชุณูุณู. ุงูุชุนููุฏ: O(nยฒ) ุจุงููุณุจุฉ ูุทูู ุงูุชุณูุณู n.",
                "problem": "ูุญุชุงุฌูู ูููู ุนูุงูุงุช ูู ุงููููุงุช ุฌูู ููุณ ุงูุฌููุฉ ุฃู ุงูุชุณูุณู ูู ููุณ ุงูููุชุ ูููุงู ููุฏุฑ ูุนูู parallel processing ูุชุณูุณูุงุช ุทูููุฉ.",
                "reference": "BERT, GPT, Transformer Encoder, Transformer Decoder",
                "category": ["mechanisms", "basic"],
                "classification": "mechanism",
                "classificationType": "basic"
            }, {
                "id": 3,
                "type": "Multi-Head Attention",
                "description": "ุนุฏุฉ ูุณุฎ ูุชูุงุฒูุฉ ูู Self-Attention ุชูุณูุท ุนูู ุฃุจุนุงุฏ ูุฎุชููุฉ ุซู ูุชู ุฏูุฌูุงุ ุจุญูุซ ูู ุฑุฃุณ (head) ูุชุนูู ููุน ูุฎุชูู ูู ุงูุนูุงูุงุช ุจูู ุงููููุงุช.",
                "rnnProblem": "RNN/LSTM ุจูุชุนูู ููุน ูุงุญุฏ ูู ุงูุนูุงูุงุช ููุท ุจูู ุงููููุงุชุ ููุด ุจููุฏุฑ ูุฑูุฒ ุนูู ุฃูุชุฑ ูู pattern ูู ููุณ ุงูููุชุ ูุฏู ุจูุญุฏ ูู ุงููุฏุฑุฉ ุนูู ููู ุงูุนูุงูุงุช ุงููุชุนุฏุฏุฉ (syntax, semantics, position).",
                "detailedSolution": "Multi-Head Attention ุจูุนูู ุนุฏุฉ ูุณุฎ ูู Attention (heads) ุจุดูู ูุชูุงุฒูุ ูู ูุงุญุฏ ุจูุชุนูู ููุน ูุฎุชูู ูู ุงูุนูุงูุงุช. ุจุนุฏ ูุฏูุ ุงูููุงุชุฌ ุจุชุชุฌูุน ุนู ุทุฑูู concat ูุถุฑุจูุง ูู ูุตูููุฉ ูุฒู W<sup>O</sup>: <span style='color:green; display: block; direction: ltr; text-align: left;'>MultiHead(Q,K,V) = Concat(headโ,...,head<sub>h</sub>)W<sup>O</sup></span> ุญูุซ <span style='color:green; display: block; direction: ltr; text-align: left;'>head<sub>i</sub> = Attention(QW<sup>Q</sup><sub>i</sub>, KW<sup>K</sup><sub>i</sub>, VW<sup>V</sup><sub>i</sub>)</span>. ูู ุฑุฃุณ ููุชูุท ุฃููุงุท ูุฎุชููุฉ ูู ุงูุจูุงูุงุช (ูุซูุงู ุงูุชุฑุงููุจ ุงููุญููุฉุ ุงููุนุงููุ ุงููููุน)ุ ููุง ูุฒูุฏ ูู ูุฏุฑุฉ ุงููููุฐุฌ ุนูู ุชูุซูู ุงูุนูุงูุงุช ุงููุนูุฏุฉ ุจูู ุงููููุงุช.",
                "problem": "ูุญุชุงุฌูู ูููู ุฃูุชุฑ ูู ููุน ุนูุงูุฉ ุจูู ุงููููุงุช ูู ููุณ ุงูููุชุ ุณูุงุก ูุนูุงูุง ุฃู ูููุนูุง ุฃู ุงูุนูุงูุฉ ุงูุชุฑููุจูุฉ.",
                "reference": "Transformer (ูุนูุงุฑ ุตูุงุนู)ุ GPT, BERT",
                "category": ["mechanisms", "basic"],
                "classification": "mechanism",
                "classificationType": "basic"
            }, {
                "id": 4,
                "type": "Cross-Attention",
                "description": "Q ูุฃุฎูุฐุฉ ูู ุชุณูุณู (ุนุงุฏุฉ Decoder) ูK/V ูุฃุฎูุฐุฉ ูู ุชุณูุณู ุขุฎุฑ ูุฎุชูู (ุนุงุฏุฉ Encoder)ุ ูุชุณูุญ ููู ุนูุตุฑ ูู ุงููDecoder ุจุงููุธุฑ ููู ุนูุงุตุฑ ุงููEncoder.",
                "rnnProblem": "ูู Encoder-Decoder RNN/LSTM ุงูุชูููุฏูุ ุงููDecoder ุบุงูุจูุง ุจูุนุชูุฏ ุนูู ุขุฎุฑ hidden state ููุท ูู ุงููEncoderุ ูุจุงูุชุงูู ุจููุณู ูุนูููุงุช ูุซูุฑุฉ ูู ุงููinput ุงูุฃุตููุ ุฎุตูุตูุง ููุฌูู ุงูุทูููุฉ.",
                "detailedSolution": "Cross-Attention ุจูุญู ุงููุดููุฉ ุฏู ุนู ุทุฑูู ุงูุณูุงุญ ููู ูููุฉ ูู ุงููDecoder ุจุฃููุง ุชุงุฎุฏ Q ูู hidden state ุงูุญุงูู ูููDecoderุ ูK/V ูู ูู hidden states ูููEncoder. ุงูุนูููุฉ: ูู ูููุฉ ูู ุงููDecoder ุจุชุญุณุจ ุงูุนูุงูุฉ ูุน ูู ุงููููุงุช ูู ุงููEncoder ูุชุฎุชุงุฑ ุงููุนูููุงุช ุงูุฃูุซุฑ ุฃูููุฉ. ุงูุตูุบุฉ ุงูุฃุณุงุณูุฉ: <span style='color:green; display: block; direction: ltr; text-align: left;'>CrossAttn(Q<sub>dec</sub>, K<sub>enc</sub>, V<sub>enc</sub>) = softmax(Q<sub>dec</sub>K<sub>enc</sub><sup>T</sup>/โd<sub>k</sub>) V<sub>enc</sub></span>. ุฏู ุจูุณูุญ ูููDecoder ุจุงููุตูู ููู ุงููุนูููุงุช ุงููููุฉ ูู ุงููEncoder ูุจุงุดุฑุฉ.",
                "problem": "ูุญุชุงุฌ ูุฑุจุท ุฌููุฉ ุฃู ุชุณูุณู ุจูุชุณูุณู ุขุฎุฑ ูุฎุชูู (ูุซู ุงููEncoder-Decoder ูู ุงูุชุฑุฌูุฉ ุงูุขููุฉ ุฃู ุงููseq2seq tasks).",
                "reference": "Transformer Decoder, T5, BARTุ ุงูุชุฑุฌูุฉ ุงูุขููุฉุ ุฃู ูููุฐุฌ Encoder-Decoder ูุนุชูุฏ ุนูู Attention.",
                "category": ["mechanisms", "basic"],
                "classification": "mechanism",
                "classificationType": "basic"
            }


            ,


            {
                "id": 5,
                "type": "Causal (Masked) Attention",
                "description": "ููุงุน ุซูุงุซู ุณููู (lower triangular mask) ูููุน ูู ูููุฉ ูู ุฑุคูุฉ ุงููููุงุช ุงููุณุชูุจููุฉ ูู ููุณ ุงูุชุณูุณูุ ููุง ูุถูู ุชูููุฏ ูุต ุฎุทูุฉ ุฎุทูุฉ.",
                "rnnProblem": "RNN/LSTM ุจุทุจูุนุชู causalุ ุฃู ูู ุฎุทูุฉ ุชุนุชูุฏ ุนูู ุงูุณุงุจูุฉ ููุทุ ูููู ุจุทูุก ุฌุฏูุง ูุฃูู ุจูุนุงูุฌ ุงููููุงุช sequential ููุง ุจูุฏุนู parallel processing.",
                "detailedSolution": "Causal Attention ุจูุทุจู mask ูุซูุซู ุนูู ุงููattention scores ูุจู ุงููsoftmax ุจุญูุซ ุฃู ูููุฉ ูู position i ุชุดูู ููุท positions โค i: <span style='color:green; display: block; direction: ltr; text-align: left;'>mask[i,j] = -โ if j > i</span>. ุฏู ูุณูุญ ูููููุฐุฌ ุจุนุฏู ุงูุงุทูุงุน ุนูู ุงููุณุชูุจู ุฃุซูุงุก ุงูุชุฏุฑูุจ ุฃู ุงูุชูููุฏุ ููู ููุณ ุงูููุช **ูุณูุญ ุจุงููparallel training** ุนูู ูุงูู ุงูุชุณูุณู ุจูุถู ุขููุฉ ุงููSelf-Attention masked.",
                "problem": "ูุญุชุงุฌูู ุชูููุฏ ูุต ุฎุทูุฉ ุฎุทูุฉ (auto-regressive) ูู ุบูุฑ ูุง ุงููููุฉ ุงูุญุงููุฉ ุชุจุต ุนูู ุงููููุงุช ุงููุณุชูุจููุฉ.",
                "reference": "GPT series, LLaMA, Transformer Decoder ููููุฐุฌุฉ ุงูุชูููุฏูุฉุ ุฃู ูููุฐุฌ autoregressive.",
                "category": ["mechanisms", "basic"],
                "classification": "mechanism",
                "classificationType": "basic"
            }, {
                "id": 6,
                "type": "Local / Sliding Window Attention",
                "description": "ุงูุงูุชุจุงู ูููู ุถูู ูุงูุฐุฉ ูุญุฏูุฏุฉ ุญูู ูู ุชูููุ ุจุญูุซ ูู ูููุฉ ุชุฑู ููุท ูุฌููุนุฉ ูุญุฏุฏุฉ ูู ุงููููุงุช ุงููุฑูุจุฉ ุจุฏู ูู ุงูุชุณูุณู.",
                "rnnProblem": "RNN/LSTM ุจูุนุงูุฌ ูู ุงูุชุณูุณู ุจุงููุงูู ุญุชู ูู ุงููุนูููุงุช ุงููููุฉ ูุฑูุจุฉุ ูุฏู ุบูุฑ ูุนุงู ุฎุตูุตูุง ููุชุณูุณูุงุช ุงูุทูููุฉ (inefficient).",
                "detailedSolution": "ูู Local / Sliding Window Attentionุ ูู token ุจูุดูู ููุท ูุงูุฐุฉ ูุญุฏุฏุฉ ุญููู (ูุซูุงู ยฑ256 tokens). ุฏู ูููู ุงูุชุนููุฏ ูู O(nยฒ) ุฅูู <span style='color:green; display: inline-block; direction: ltr;'>O(nรw)</span> ุญูุซ w ุญุฌู ุงููุงูุฐุฉ ุงูุซุงุจุชุ ูุน ุงูุญูุงุธ ุนูู ุงูุณูุงู ุงููุญูู. ูููู ุฏูุฌู ูุน global attention ูุชุบุทูุฉ ุงููlong-range dependencies ุฅุฐุง ุงุญุชุฌูุงุ ููุง ูุนุทู ุชูุงุฒู ุจูู ุงูููุงุกุฉ ูุงููุฏุฑุฉ ุนูู ุงูุชุนุงูู ูุน ุงูุชุณูุณูุงุช ุงูุทูููุฉ.",
                "problem": "ูุญุชุงุฌ ูุฑูุฒ ุนูู ุงูุณูุงู ุงููุญูู ุจููุงุกุฉ ุนุงููุฉุ ูุน ุชูููู ุงูุชุนููุฏ ุงูุญุณุงุจู ุจุงููุณุจุฉ ููุชุณูุณูุงุช ุงูุทูููุฉ.",
                "reference": "Longformer (2020), Sparse Transformer, BigBird",
                "category": ["mechanisms", "sparse"],
                "classification": "mechanism",
                "classificationType": "sparse"
            }

            , {
                "id": 7,
                "type": "Strided / Dilated Attention",
                "description": "ุงูุงูุชุจุงู ูุชู ุนุจุฑ ุฎุทูุงุช ูุชุจุงุนุฏุฉ (ูู k ุชููู)ุ ุจุญูุซ ูู ูููุฉ ุชุฑู ูููุงุช ูุชุจุงุนุฏุฉ ุจุฏูุงู ูู ูู ุงูุชุณูุณู.",
                "rnnProblem": "RNN/LSTM ูุงุฒู ููุฑ ุนูู ูู ุงููsteps ุนุดุงู ููุตู ููุนูููุฉ ุจุนูุฏุฉุ ูุฏู ุจูุถุนู gradient ููุจุทุฆ ุงูุชุนููุ ุฎุตูุตูุง ูุน ุงูุชุณูุณูุงุช ุงูุทูููุฉ.",
                "detailedSolution": "ูู Strided / Dilated Attentionุ ูู token ูุดูู ููุท ูู k tokens (stride = k) ุจุฏูุงู ูู ูู ุงููููุงุช. ุฏู ูุณูุญ ูููููุฐุฌ ุจุงููุตูู ูููุณุงูุงุช ุงูุจุนูุฏุฉ ุจุชูููุฉ ุฃูู: <span style='color:green; display: inline-block; direction: ltr;'>O(nยฒ/k)</span>. ูููู ุงุณุชุฎุฏุงู ุนุฏุฉ strides ูุฎุชููุฉ ุนุจุฑ layers ูุชุนุฏุฏุฉ ูุชุนูู multi-scale patternsุ ุจุญูุซ ุงููููุฐุฌ ููุชูุท ุงูุนูุงูุงุช ุงููุตูุฑุฉ ูุงูุทูููุฉ ุจููุงุกุฉ ุนุงููุฉ.",
                "problem": "ูุญุชุงุฌ ุงููุตูู ุฅูู ูููุงุช ุจุนูุฏุฉ ูู ุงูุชุณูุณู ุจุชูููุฉ ุฃูู ูู O(nยฒ)ุ ูุน ุงูุญูุงุธ ุนูู capture ูููlong-range dependencies.",
                "reference": "Sparse Transformer (OpenAI, 2019), Long-Range Transformer",
                "category": ["mechanisms", "sparse"],
                "classification": "mechanism",
                "classificationType": "sparse"
            }


            , {
                "id": 8,
                "type": "Global Attention",
                "description": "ุชูููุงุช ูุญุฏุฏุฉ (special tokens) ูุงุฏุฑุฉ ุนูู ุฑุคูุฉ ูู ุงูุชุณูุณู ุจุฃููููุ ูุงูุนูุณ ุตุญูุญุ ุจูููุง ุจุงูู ุงูุชูููุงุช ุชุนูู Local Attention.",
                "rnnProblem": "RNN/LSTM ูููููุด ููุฑุฉ global tokensุ ูู ุงููุนูููุงุช ูุงุฒู ุชูุฑ ุนุจุฑ ุงููhidden state ุงููุถุบูุทุ ูุฏู ูููู ูุถุนู ุฃู ููุณู ุงููุนูููุงุช ุงูุทูููุฉ.",
                "detailedSolution": "ูู Global Attentionุ ูุญุฏุฏ ุจุนุถ ุงููtokens (ูุซู [CLS] ุฃู ุฃูู token ูู ูู paragraph) ุจุญูุซ ุชุนูู full attention ููู ุงูุชุณูุณูุ ูุงูุนูุณ ุตุญูุญ: ุฃู ูููุฉ ูู ุงูุชุณูุณู ูููููุง ุงูุชูุงุตู ูุน ูุฐู ุงููglobal tokens. ุฏู ูุฌุนููุง ุจูุซุงุจุฉ global memory ูุชุงุญุฉ ูู ุฃู ููุงู ูู ุงูุชุณูุณูุ ุจูููุง ุจุงูู ุงูุชูููุงุช ุชุชุนุงูู ุถูู local context ููุทุ ููุง ูููู ุงูุชุนููุฏ ุงูุญุณุงุจู ููุงุฑูุฉ ุจุงููfull attention ููู ุงูุชูููุงุชุ ููุญุงูุธ ุนูู ุงููุฏุฑุฉ ุนูู ุงููุตูู ุฅูู ูุนูููุงุช ูููุฉ ุนุจุฑ ุงูุชุณูุณู.",
                "problem": "ูุญุชุงุฌ ุฃู ุจุนุถ ุงููtokens ุงููููุฉ ุชุฑู ูู ุงููููุงุช ูู ุงูุชุณูุณู ูุงูุนูุณุ ูุซู ุญุงูุงุช ุงููclassification ุฃู summarization.",
                "reference": "Longformer (2020), BigBird (2020), ุฃู ูููุฐุฌ sparse attention ูุณุชุฎุฏู global tokens.",
                "category": ["mechanisms", "sparse"],
                "classification": "mechanism",
                "classificationType": "sparse"
            }, {
                "id": 8,
                "type": "Global Attention",
                "description": "ุชูููุงุช ูุญุฏุฏุฉ (special tokens) ูุงุฏุฑุฉ ุนูู ุฑุคูุฉ ูู ุงูุชุณูุณู ุจุฃููููุ ูุงูุนูุณ ุตุญูุญุ ุจูููุง ุจุงูู ุงูุชูููุงุช ุชุนูู Local Attention.",
                "rnnProblem": "RNN/LSTM ูููููุด ููุฑุฉ global tokensุ ูู ุงููุนูููุงุช ูุงุฒู ุชูุฑ ุนุจุฑ ุงููhidden state ุงููุถุบูุทุ ูุฏู ูููู ูุถุนู ุฃู ููุณู ุงููุนูููุงุช ุงูุทูููุฉ.",
                "detailedSolution": "ูู Global Attentionุ ูุญุฏุฏ ุจุนุถ ุงููtokens (ูุซู [CLS] ุฃู ุฃูู token ูู ูู paragraph) ุจุญูุซ ุชุนูู full attention ููู ุงูุชุณูุณูุ ูุงูุนูุณ ุตุญูุญ: ุฃู ูููุฉ ูู ุงูุชุณูุณู ูููููุง ุงูุชูุงุตู ูุน ูุฐู ุงููglobal tokens. ุฏู ูุฌุนููุง ุจูุซุงุจุฉ global memory ูุชุงุญุฉ ูู ุฃู ููุงู ูู ุงูุชุณูุณูุ ุจูููุง ุจุงูู ุงูุชูููุงุช ุชุชุนุงูู ุถูู local context ููุทุ ููุง ูููู ุงูุชุนููุฏ ุงูุญุณุงุจู ููุงุฑูุฉ ุจุงููfull attention ููู ุงูุชูููุงุชุ ููุญุงูุธ ุนูู ุงููุฏุฑุฉ ุนูู ุงููุตูู ุฅูู ูุนูููุงุช ูููุฉ ุนุจุฑ ุงูุชุณูุณู.",
                "problem": "ูุญุชุงุฌ ุฃู ุจุนุถ ุงููtokens ุงููููุฉ ุชุฑู ูู ุงููููุงุช ูู ุงูุชุณูุณู ูุงูุนูุณุ ูุซู ุญุงูุงุช ุงููclassification ุฃู summarization.",
                "reference": "Longformer (2020), BigBird (2020), ุฃู ูููุฐุฌ sparse attention ูุณุชุฎุฏู global tokens.",
                "category": ["mechanisms", "sparse"],
                "classification": "mechanism",
                "classificationType": "sparse"
            }


            , {
                "id": 9,
                "type": "Random Attention",
                "description": "ูู ุชููู ูุฎุชุงุฑ ุนุฏุฏ ุซุงุจุช ูู ุงูุชูููุงุช ุงูุฃุฎุฑู ุจุดูู ุนุดูุงุฆู ููุงูุชุจุงู ุฅูููุงุ ูุชูููุฑ ุชููุน ูุชุบุทูุฉ ุฅุถุงููุฉ ูู ุงููุนูููุงุช.",
                "rnnProblem": "RNN/LSTM ุจูุชุจุน ููุณ ุงููุณุงุฑ ุฏุงุฆููุง ุฃุซูุงุก ูุนุงูุฌุฉ ุงูุชุณูุณูุ ููุง ุจูุญุชููุด ุนูู randomness ุทุจูุนูุ ูุฏู ูููู ูููู ุงูุชุนููู ููุฒูุฏ overfitting ูููpatterns ุงููุญุฏุฏุฉ.",
                "detailedSolution": "ูู Random Attentionุ ูู token ูุฎุชุงุฑ r tokens ุนุดูุงุฆูุฉ ููุงูุชุจุงู ุฅูููุง ุจุงุณุชุฎุฏุงู random sampling ูุน fixed seed ูุถูุงู reproducibility. ุฏู ุจููุณุฑ ุงูุชูุงุซู ุจูู ุงููุณุงุฑุงุช ุงููุฎุชููุฉุ ูููุน overfittingุ ููุณูุญ ูููุนูููุงุช ุจุงูุงูุชุดุงุฑ ุนุจุฑ network ุนุจุฑ ูุณุงุฑุงุช ูุชููุนุฉ. ูููู ุฏูุฌู ูุน Local ุฃู Global Attention ูุชุบุทูุฉ ูู ุงูุชุณูุณู ุจููุงุกุฉ ูุน ุฅุถุงูุฉ diversity.",
                "problem": "ูุญุชุงุฌ ุฅุถุงูุฉ diversity ููุงูุชุจุงู ูููุน overfitting ูููpatterns ุงููุญุฏุฏุฉุ ูุน ุชุญุณูู ุงูุชุดุงุฑ ุงููุนูููุงุช ุนุจุฑ ุงูุดุจูุฉ.",
                "reference": "BigBird (2020), Sparse Transformer, ุฃู ูููุฐุฌ Sparse Attention ูุณุชุฎุฏู random connections.",
                "category": ["mechanisms", "sparse"],
                "classification": "mechanism",
                "classificationType": "sparse"
            }


            , {
                "id": 10,
                "type": "Block / Chunked Attention",
                "description": "ุชูุณูู ุงูุชุณูุณู ุฅูู ูุชู (blocks) ูุงูุงูุชุจุงู ุฏุงุฎู ูู block ูุจูู ุงููุชู ุงููุฌุงูุฑุฉุ ูุชูููู ุงูุชุนููุฏ ูุน ุงูุญูุงุธ ุนูู ุงูุณูุงู ุงููุญูู ูุงููุชุฌุงูุฑ.",
                "rnnProblem": "RNN/LSTM ุจูุนุงูุฌ ุงูุชุณูุณู ููู ุฎุทูุฉ ุฎุทูุฉุ ูุฏู ุจูุตุนุจ ุงูุชุนุงูู ูุน sequences ุทูููุฉ ุฌุฏูุง ููุฒูุฏ ุงูููุช ุงููุงุฒู ููุญุณุงุจ.",
                "detailedSolution": "ูู Block / Chunked Attentionุ ููุณู ุงููsequence ุฅูู blocks (ูุซูุงู ูู block = 64 tokens). ูููู ุจุนูู attention ุฏุงุฎู ูู block ุจุงูุฅุถุงูุฉ ุฅูู attention ุจูู blocks ุงููุฌุงูุฑุฉ ูุถูุงู continuity. ูุฐุง ูููู ุงูุชุนููุฏ ูู O(nยฒ) ุฅูู <span style='color:green; display: inline-block; direction: ltr;'>O(nยฒ/b)</span> ุญูุซ b ุญุฌู block. ูููู ุงุณุชุฎุฏุงู blocks ูุชุฏุงุฎูุฉ (overlapping) ูุถูุงู ุนุฏู ููุฏุงู ุงูุณูุงู ุนูุฏ ุงูุญุฏูุฏ ุจูู blocks.",
                "problem": "ูุญุชุงุฌ ูุนุงูุฌุฉ sequences ุทูููุฉ ุจููุงุกุฉ ุนู ุทุฑูู ุชูุณูููุง ุฅูู ูุชู ูุน ุงูุญูุงุธ ุนูู ุงูุฑุจุท ุจูู ุงููููุงุช ุงููุชุฌุงูุฑุฉ.",
                "reference": "BigBird (2020), Block-Sparse Attention, Long-Range Transformers",
                "category": ["mechanisms", "sparse"],
                "classification": "mechanism",
                "classificationType": "sparse"
            }


            , {
                "id": 11,
                "type": "Fixed Patterns Attention",
                "description": "ุงุณุชุฎุฏุงู ุฃููุงุท ุงูุชุจุงู ุซุงุจุชุฉ ูุญุฏุฏุฉ ูุณุจููุง (ูุซู ุงูุตูููุ ุงูุฃุนูุฏุฉุ ุฃู ุงูุฃูุทุงุฑ) ูุงุณุชุบูุงู ุงูุจููุฉ ุงูุทุจูุนูุฉ ููุจูุงูุงุช ุงูููุธูุฉ ููุฏุณูุงู.",
                "rnnProblem": "RNN/LSTM ูุด ูุงุฏุฑ ูุณุชููุฏ ูู ุงูุจููุฉ ุงูููุฏุณูุฉ ููุจูุงูุงุช (ูุซู ุงูุตูุฑ 2D ุฃู ุงูุฌุฏุงูู)ุ ูุฃูู ูุนุงูุฌ ุงูุชุณูุณู ุจุดูู linear ููุท ููุถูุน ุงููุนูููุงุช ุงููููููุฉ.",
                "detailedSolution": "ูู Fixed Patterns Attentionุ ูุญุฏุฏ patterns ูุญุฏุฏุฉ ูุณุจููุง ูุซู: \n- row attention: ูู ุนูุตุฑ ูุดูู ุงูุนูุงุตุฑ ูู ููุณ ุงูุตู\n- column attention: ูู ุนูุตุฑ ูุดูู ุงูุนูุงุตุฑ ูู ููุณ ุงูุนููุฏ\n- diagonal attention: ูู ุนูุตุฑ ูุดูู ุงูุนูุงุตุฑ ุนูู ุงููุทุฑ\nูุฐู ุงูุฃููุงุท ุชุณุชุบู ุงูุจููุฉ ุงูุทุจูุนูุฉ ููุจูุงูุงุช ุงูููุธูุฉุ ูุชููู ุงูุชุนููุฏ ุงูุญุณุงุจู ุจุดูู ูุจูุฑ ููุงุฑูุฉ ุจุงููfull attention ูุฌููุน ุงูุนูุงุตุฑุ ูุน ุงูุญูุงุธ ุนูู ุงููุนูููุงุช ุงููููููุฉ ุงููููุฉ.",
                "problem": "ูุญุชุงุฌ ุงุณุชุฎุฏุงู patterns ุซุงุจุชุฉ ูุญุฏุฏุฉ ูุณุจููุง ูุชูููู ุงูุชุนููุฏ ุงูุญุณุงุจู (sparsity) ูุงูุงุณุชูุงุฏุฉ ูู ุงูุจููุฉ ุงูููุฏุณูุฉ ููุจูุงูุงุช.",
                "reference": "Sparse Transformer (OpenAI, 2019), Long-Range Sparse Transformers",
                "category": ["mechanisms", "sparse"],
                "classification": "mechanism",
                "classificationType": "sparse"
            }, {
                "id": 12,
                "type": "Sparse Top-K Attention",
                "description": "ูู token ูุฑูุฒ ุนูู ุฃุนูู K ููู ูู attention scores ููุท ุจุดูู ุฏููุงููููุ ููุง ูููู ุงูุญุณุงุจุงุช ููุฑูุฒ ุนูู ุงูุนูุงุตุฑ ุงูุฃูู.",
                "rnnProblem": "RNN/LSTM ุจูุฏู ููุณ ุงูุงูุชูุงู ููู ุงููstepsุ ููุด ุจูุฑูุฒ ุนูู ุฃูู ุงูุนูุงุตุฑ ูู ุงูุชุณูุณูุ ููุง ูููู ุงูููุงุกุฉ ุฎุตูุตูุง ููุชุณูุณูุงุช ุงูุทูููุฉ.",
                "detailedSolution": "ูู Sparse Top-K Attentionุ ูุญุณุจ ูู attention scores ุฃููุงูุ ุซู ูุฃุฎุฐ ููุท ุฃุนูู K ููู ููุถุน ุจุงูู ุงูููู = 0 ูุจู ุชุทุจูู softmax. ูุฐุง ูุณูุญ ูููููุฐุฌ ุจุงูุชุฑููุฒ ุนูู ุฃูู K tokens ุจุดูู adaptive ุญุณุจ ูุญุชูู ุงูุชุณูุณู. โ๏ธ ูููbackward passุ ูููู ุงุณุชุฎุฏุงู straight-through estimator ุฃู softmax ูุน temperature ููุญูุงุธ ุนูู differentiability ุฃุซูุงุก ุงูุชุฏุฑูุจ.",
                "problem": "ูุญุชุงุฌ ุชูููู ุงูุญุณุงุจุงุช ุนูู ุงูุชุณูุณู ุงูุทููู ูุงูุชุฑููุฒ ููุท ุนูู ุฃูู ุงููููุงุช ุฃู ุงูุนูุงุตุฑ ูู ุงููsequence.",
                "reference": "Top-K Sparse Attention (Adaptive Sparsity), Long-Range Transformers",
                "category": ["mechanisms", "sparse"],
                "classification": "mechanism",
                "classificationType": "sparse"
            }, {
                "id": 13,
                "type": "LSH (Locality Sensitive Hashing) Attention",
                "description": "ุชูุฑูุจ ุงูุชุดุงุจู ุจูู ุงูุชูููุงุช ุนุจุฑ hashing ูุชุฌููุน ุงูุชูููุงุช ุงููุชุดุงุจูุฉ ูุนูุงุ ุจุญูุซ ูุชู ุนูู attention ููุท ุถูู ุงูุชูููุงุช ุงููุชุดุงุจูุฉ.",
                "rnnProblem": "RNN/LSTM ูุงุฒู ูุนุงูุฌ ูู ุงูุชุณูุณู ุฎุทูุฉ ุฎุทูุฉุ ุญุชู ูู ููู tokens ูุชุดุงุจูุฉ ูููู ุชุฌููุนููุ ููุง ูุฒูุฏ ุงูููุช ูุงูุชุนููุฏ ุงูุญุณุงุจู ูุน ุงูุชุณูุณูุงุช ุงูุทูููุฉ.",
                "detailedSolution": "ูู LSH Attentionุ ูุณุชุฎุฏู Locality Sensitive Hashing ูุชุญููู ุงููqueries ูุงููkeys ุฅูู hash buckets. ุงูุชูููุงุช ุงูุชู ุชูุน ูู ููุณ ุงููbucket ุฃู ูู buckets ูุฌุงูุฑุฉ ุชููู ุจุนูู attention ูุน ุจุนุถูุง ููุท. ูุฐุง ูููู ุงูุชุนููุฏ ุงูุญุณุงุจู ูู O(nยฒ) ุฅูู <span style='color:green; display: inline-block; direction: ltr;'>O(n log n)</span> ูู ุงููุชูุณุทุ ูุน ุงูุญูุงุธ ุนูู ูุฏุฑุฉ ุงููููุฐุฌ ุนูู ุงูุชุฑููุฒ ุนูู ุงูุชูููุงุช ุงููุชุดุงุจูุฉ ุจุดูู ูุนุงู.",
                "problem": "ูุญุชุงุฌ ุงูุนุซูุฑ ุนูู ุงูุชูููุงุช ุงููุชุดุงุจูุฉ ุจุณุฑุนุฉ ูุงูุนูู ุนูููุง ููุทุ ูุชุณุฑูุน ุงูุงูุชุจุงู ูู ุงูุชุณูุณูุงุช ุงูุทูููุฉ.",
                "reference": "Reformer (Kitaev et al., 2020), Long-Range Transformers",
                "category": ["mechanisms", "sparse"],
                "classification": "mechanism",
                "classificationType": "sparse"
            }, {
                "id": 14,
                "type": "Clustering-Based Attention",
                "description": "ุชุฌููุน ุงูุชูููุงุช ุงููุชุดุงุจูุฉ ูุนูุง (semantically) ูุงูุงูุชุจุงู ุฏุงุฎู ูู ูุฌููุนุฉ (cluster) ูุชูููู ุงูุญุณุงุจุงุช ูุน ุงูุญูุงุธ ุนูู ุชูุซูู ุงูุนูุงูุงุช ุงููููุฉ.",
                "rnnProblem": "RNN/LSTM ูุด ูุงุฏุฑ ูุฌูุน tokens ูุชุดุงุจูุฉ semanticallyุ ูุฃูู ูุนุงูุฌ ุงูุชุณูุณู ุฎุทูุฉ ุฎุทูุฉ ููุง ุจูุณุชููุฏ ูู ุงูุจููุฉ ุงูุฏูุงููุฉ ุจูู ุงููููุงุช.",
                "detailedSolution": "ูู Clustering-Based Attentionุ ูุณุชุฎุฏู soft clustering ุนูู keys/queries ุจุงุณุชุฎุฏุงู learnable cluster centers. ูู cluster ูุนูู attention ุฏุงุฎูููุง (internal attention)ุ ููููู ุฅุถุงูุฉ cross-cluster attention ูุญุฏูุฏ ุฅุฐุง ุงุญุชุฌูุง ูุฑุจุท ุงููุนูููุงุช ุจูู ุงููุฌููุนุงุช. soft assignment ููููุฐ ุนุงุฏุฉู ุจุงุณุชุฎุฏุงู Gumbel-Softmax ุฃู ุชูููุงุช ูุดุงุจูุฉ ูุถูุงู differentiability ุฃุซูุงุก ุงูุชุฏุฑูุจุ ุจุฏู hard K-means assignment. ูุฐุง ูููู ุงูุชุนููุฏ ุงูุญุณุงุจู ููุนุทู ูุฏุฑุฉ ุงููููุฐุฌ ุนูู ุงูุชุฑููุฒ ุนูู ุงูุชูููุงุช ุงููุชุดุงุจูุฉ semantically.",
                "problem": "ูุญุชุงุฌ ุชุฌููุน tokens ูุชุดุงุจูุฉ semantically ูุงูุงูุชุจุงู ุฏุงุฎู ูู ูุฌููุนุฉ ูุชูููู ุงูุญุณุงุจุงุช ูุชุญุณูู ุงูุชุนูู.",
                "reference": "Clustered Attention, Routing Transformers (2020), Long-Range Transformers",
                "category": ["mechanisms", "sparse"],
                "classification": "mechanism",
                "classificationType": "sparse"
            }, {
                "id": 15,
                "type": "Axial Attention",
                "description": "ุงูุชุจุงู ููุทุจู ุนูู ูู ุจุนุฏ ูู ุฃุจุนุงุฏ ุงูุจูุงูุงุช ุจุดูู ูููุตูุ ูุงุณุชุบูุงู ุงูุจููุฉ ูุชุนุฏุฏุฉ ุงูุฃุจุนุงุฏ ุฏูู ุฒูุงุฏุฉ ูุจูุฑุฉ ูู ุงูุชุนููุฏ ุงูุญุณุงุจู.",
                "rnnProblem": "RNN/LSTM ูุด ูุตูู ููุชุนุงูู ูุน ุงูุจูุงูุงุช ูุชุนุฏุฏุฉ ุงูุฃุจุนุงุฏุ ุนุงุฏุฉ ุจูุญูููุง ุฅูู 1D sequence ูุจุงูุชุงูู ุจูุถูุน ุงูุจููุฉ ุงูููุงููุฉ ุฃู ุงูุฒูููุฉ ููุจูุงูุงุช.",
                "detailedSolution": "ูู Axial Attentionุ ูุทุจู ุงูุงูุชุจุงู ุนูู ูู ุจุนุฏ ูู ุฃุจุนุงุฏ ุงูุจูุงูุงุช ุจุดูู ูููุตู. ูุซุงู ููุตูุฑ (HรW): ุจุฏู ุนูู attention ุนูู ูู ุงููpixels ูุฑุฉ ูุงุญุฏุฉ (O(HยฒWยฒ))ุ ูููู ุฃููุงู ุจุงููattention ุนูู ูู row (O(HWยฒ)) ุซู ุนูู ูู column (O(HยฒW)). ุจุฐูู ูุตุจุญ ุงูุชุนููุฏ ุงูููู <span style='color:green; display: inline-block; direction: ltr;'>O(HW(H+W))</span> ุจุฏู O(HยฒWยฒ). ููุณ ุงูููุฑุฉ ูููู ุชุทุจูููุง ุนูู ุจูุงูุงุช 3D (ูุซู ุงูููุฏูู) ุจุญูุซ ูุทุจู attention ุนูู timeรheightรwidth ุจุดูู ูููุตู ููู ุจุนุฏ.",
                "problem": "ูุญุชุงุฌ ูุนุงูุฌุฉ ุจูุงูุงุช ูุชุนุฏุฏุฉ ุงูุฃุจุนุงุฏ (ุตูุฑุ ููุฏููุ ุฃู ุฃู tensor ุนุงูู ุงูุฃุจุนุงุฏ) ุจููุงุกุฉ ูุน ุงูุญูุงุธ ุนูู ุงูุนูุงูุงุช ุงูููุงููุฉ ุฃู ุงูุฒูููุฉ.",
                "reference": "Axial Transformer (Ho et al., 2019), Axial-DeepLab",
                "category": ["mechanisms"],
                "classification": "mechanism",
                "classificationType": "advanced"
            }, {
                "id": 16,
                "type": "Hierarchical Attention",
                "description": "ุงูุชุจุงู ูุชุนุฏุฏ ุงููุณุชููุงุช (coarse-to-fine) ูุณูุญ ูููููุฐุฌ ุจุงูุชุฑููุฒ ุนูู ุงูุณูุงู ุงูุนุงููู ูุงูุฌุฒุฆูุงุช ุงููุญููุฉ ูู ููุณ ุงูููุช.",
                "rnnProblem": "RNN/LSTM ุจูุนุงูุฌ ูู tokens ุจููุณ ุงููุณุชููุ ููุง ุจููุฏุฑุด ููุซู hierarchy ุฃู ูุณุชููุงุช ูุฎุชููุฉ ูู ุงูุชุฌุฑูุฏ ุฏุงุฎู ุงููุต ุฃู ุงูุจูุงูุงุช ุงูุทูููุฉ.",
                "detailedSolution": "ูู Hierarchical Attentionุ ููุณู ุงูุจูุงูุงุช ุฃู ุงููุตูุต ุฅูู ูุณุชููุงุช: \n- ุฃูู ูุณุชูู: chunks ูุจูุฑุฉ (document-level) ูุชุญุฏูุฏ ุงูุณูุงู ุงูุนุงู.\n- ุซุงูู ูุณุชูู: paragraphs ูุชุญุฏูุฏ ุงูุณูุงู ุงููุชูุณุท.\n- ุซุงูุซ ูุณุชูู: sentences ุฃู tokens ูุชุญุฏูุฏ ุงูุชูุงุตูู ุงูุฏูููุฉ.\nุจูุฐุง ุงูุดููุ ูููู ูููููุฐุฌ ุฑุคูุฉ ุงููglobal context ูุงููlocal details ูู ููุณ ุงูููุช ุจุงุณุชุฎุฏุงู ุจููุฉ ูุฑููุฉุ ููุง ูุญุณู ุงููุฏุฑุฉ ุนูู ุชูุซูู ุงููุนูููุงุช ุงูุทูููุฉ ูุงููุนูุฏุฉ.",
                "problem": "ูุญุชุงุฌ ุฃู ูููู ุงูุงูุชุจุงู ุนูู ูุณุชููุงุช ูุฎุชููุฉ ูู ุงูุชุฌุฑูุฏุ ุจุญูุซ ูุณุชููุฏ ูู ุงูุณูุงู ุงูุนุงููู ูุงูุฌุฒุฆูุงุช ุงููุญููุฉ ูุนูุง.",
                "reference": "Hierarchical Attention Networks (Yang et al., 2016), multi-level attention models",
                "category": ["mechanisms"],
                "classification": "mechanism",
                "classificationType": "advanced"
            }


            , {
                "id": 17,
                "type": "Learnable Sparse Patterns",
                "description": "ุชุนูู ุฃููุงุท ุงูุชูุงุซุฑ (sparsity) ุฃุซูุงุก ุงูุชุฏุฑูุจ ุจุทุฑููุฉ ูุงุจูุฉ ููุงุดุชูุงูุ ุจุญูุซ ููุฑุฑ ุงููููุฐุฌ ุจููุณู ุฃู ุงููconnections ูุฌุจ ุฃู ุชููู ูุนุงูุฉ ูุฃููุง ุชูุนุทููู.",
                "rnnProblem": "RNN/LSTM ูุด sparse ุจุทุจูุนุชูุ ุจูุนุงูุฌ ูู ุงูุนูุงุตุฑ ูู ุงูุชุณูุณู ุจููุณ ุงูุทุฑููุฉ ููุง ูููุฒ ุจูู ุงูููู ูุบูุฑ ุงููููุ ููุง ูุฒูุฏ ุงูุชุนููุฏ ุงูุญุณุงุจู ููุญุฏ ูู ุงูููุงุกุฉ.",
                "detailedSolution": "ูู Learnable Sparse Patternsุ ูุถูู learnable mask parameters ุนูู ูุตูููุฉ ุงููattention. ุฎูุงู ุงูุชุฏุฑูุจุ ุงููmask ูุชุนูู ุฃู ูุฌุนู ุจุนุถ ุงููconnections = 0 ูุชูููู ุงูุชุนููุฏ ูุงูุชุฑููุฒ ุนูู ุงูุนูุงูุงุช ุงููููุฉ. ูุชู ุงุณุชุฎุฏุงู Gumbel-Softmax ุฃู Concrete distribution ูุถูุงู differentiabilityุ ููููู ุฅุถุงูุฉ L0 regularization ูุชุดุฌูุน sparsity. ุจูุฐู ุงูุทุฑููุฉุ ุงููsparsity pattern ููุณู ูุชุนูู ูู ุงูุจูุงูุงุช ุจุทุฑููุฉ adaptive ููุนุงูุฉ.",
                "problem": "ูุญุชุงุฌ ุฃู ูุชุนูู ุงููููุฐุฌ ููุณู ุฃููุงุท sparsity ุงูููุงุณุจุฉ ูู ุงูุจูุงูุงุชุ ุจุฏู ุชุญุฏูุฏูุง ูุฏููุงู.",
                "reference": "Dynamic Sparse Attention, Differentiable Masking, Learnable Sparse Transformers",
                "category": ["mechanisms", "sparse"],
                "classification": "mechanism",
                "classificationType": "sparse"
            }


            , {
                "id": 18,
                "type": "Linear Attention",
                "description": "ุงุณุชุฎุฏุงู Kernel Trick ูุชุญููู ุงูุชุนููุฏ ุงูุญุณุงุจู ูู O(nยฒ) ุฅูู ุฎุทู O(n)ุ ูุน ุงูุญูุงุธ ุนูู ุงููุฏุฑุฉ ุนูู parallel processing.",
                "rnnProblem": "RNN/LSTM ุฎุทู O(n) ููู sequentialุ ุจูููุง Self-Attention ุงูุชูููุฏู O(nยฒ) ูููู parallelizableุ ูุฐูู ูุญุชุงุฌ ูุฒูุฌ ูู ููุงุกุฉ ุงููRNN ูุฎูุงุต ุงููAttention.",
                "detailedSolution": "ูู Linear Attentionุ ูุณุชุฎุฏู kernel trick: <span style='color:green; display: block; direction: ltr; text-align: left;'>ฯ(Q)(ฯ(K)<sup>T</sup>V)</span> ุจุฏู <span style='color:green; display: block; direction: ltr; text-align: left;'>(QK<sup>T</sup>)V</span>. ุฃููุงู ูุญุณุจ (ฯ(K)<sup>T</sup>V) ุจุชุนููุฏ O(ndยฒ)ุ ุซู ูุถุฑุจ ฯ(Q) ูู ุงููุชูุฌุฉ O(ndยฒ). ุจุงูุชุงูู ูุตุจุญ ุงูุชุนููุฏ ุงูููู <span style='color:green; display: inline-block; direction: ltr;'>O(ndยฒ) โ O(n)</span> ูููd ุซุงุจุชุ ูุน ุฅููุงููุฉ parallel processing ุนูู ูุงูู ุงูุชุณูุณูุ ููุง ูุฌูุน ุจูู ุงูุณุฑุนุฉ ูุงูููุงุกุฉ ุงูุญุณุงุจูุฉ.",
                "problem": "ูุญุชุงุฌ ุชูููู ุงูุชุนููุฏ ูู O(nยฒ) ุฅูู O(n) ูุน ุงูุงุญุชูุงุธ ุจุงููุฏุฑุฉ ุนูู parallel processing ูู ุชุณูุณูุงุช ุทูููุฉ.",
                "reference": "Linear Transformer (Katharopoulos et al., 2020), Kernel-based Attention, Efficient Transformers",
                "category": ["mechanisms"],
                "classification": "mechanism",
                "classificationType": "advanced"
            },

            {
                "id": 19,
                "type": "Performer Attention",
                "description": "ุชูุฑูุจ Self-Attention ุจุงุณุชุฎุฏุงู Random Feature Mapping (FAVOR+) ูุชูููู ุงูุชุนููุฏ ุงูุญุณุงุจู ูุน ุงูุญูุงุธ ุนูู ุฏูุฉ ุนุงููุฉ.",
                "rnnProblem": "RNN/LSTM ุชูุงุฌู ููุณ ุงููุดููุฉ: sequential processing ุฃู Self-Attention ุงูุชูููุฏู O(nยฒ)ุ ููุง ูุตุนุจ ุงูุชุนุงูู ูุน sequences ุทูููุฉ ุฌุฏูุง.",
                "detailedSolution": "ูู Performer Attentionุ ูุณุชุฎุฏู random Fourier features ุนุจุฑ FAVOR+ algorithm ูุชูุฑูุจ softmax kernel: <span style='color:green; display: block; direction: ltr; text-align: left;'>softmax(QK<sup>T</sup>/โd) โ ฯ(Q)ฯ(K)<sup>T</sup></span> ุญูุซ ฯ ูู positive random features. ูุฐุง ูุณูุญ ุจุญุณุงุจ attention ุจุชุนููุฏ O(n) ุจุฏู O(nยฒ)ุ ูุน ุงูุญูุงุธ ุนูู unbiased approximation ูููsoftmax attentionุ ูุจุงูุชุงูู ูููู ุงูุชุนุงูู ูุน sequences ุทูููุฉ ุฌุฏูุง ุจููุงุกุฉ ุนุงููุฉ.",
                "problem": "ูุญุชุงุฌ scalable Self-Attention ูุชุณูุณูุงุช ุทูููุฉ ุฌุฏูุง ูุน ุงูุญูุงุธ ุนูู ุฏูุฉ ุงููsoftmax ุงูุชูููุฏูุฉ.",
                "reference": "Performer (Choromanski et al., 2021), FAVOR+ algorithm, Efficient Transformers",
                "category": ["mechanisms"],
                "classification": "mechanism",
                "classificationType": "advanced"
            },



            {
                "id": 20,
                "type": "Relative Positional Attention",
                "description": "ุฅุถุงูุฉ ุชุญูุฒ ูุนุชูุฏ ุนูู ุงููุณุงูุฉ ุงููุณุจูุฉ ุจูู ุงูุชูููุงุชุ ุจุญูุซ ูุชููู ุงููููุฐุฌ ูู ุงูุชุนูู ุจูุงุกู ุนูู ุงููุณุงูุฉ ุจูู ุงููููุงุช ุจุฏู ุงููabsolute positions ููุท.",
                "rnnProblem": "RNN/LSTM ุนูุฏู ูุนูููุงุช positional ุถูููุฉ ูุฃูู sequentialุ ููู ูุน ูุฑูุฑ ุงูููุช (long sequences) ูุฐู ุงููุนูููุงุช ุจุชุถุนู ุฃู ุชููุณูุ ุฎุตูุตูุง ููุณูุงู ุงูุทููู.",
                "detailedSolution": "ูู Relative Positional Attentionุ ุจุฏู ุงุณุชุฎุฏุงู absolute position embeddingsุ ูุถูู bias ุนูู attention scores ุญุณุจ ุงููุณุงูุฉ ุงููุณุจูุฉ ุจูู ุงูุชูููุงุช (i-j): <span style='color:green; display: block; direction: ltr; text-align: left;'>score<sub>ij</sub> = q<sub>i</sub>ยทk<sub>j</sub> + q<sub>i</sub>ยทr<sub>(i-j)</sub></span> ุญูุซ r ูู ุชูุซูู learnable ูููุณุงูุฉ ุงููุณุจูุฉ. ูุฐุง ูุณูุญ ูููููุฐุฌ ุจุชุนูู patterns ุชุนุชูุฏ ุนูู ุงููุณุงูุฉ ุงููุณุจูุฉ ุจูู ุงููููุงุช ูููุณ ุนูู ุงูููุถุน ุงููุทูู ููุทุ ููุง ูุญุณู generalization ุนูู sequences ุทูููุฉ ููุนุฒุฒ ููู ุงูุชุณูุณู.",
                "problem": "ูุญุชุงุฌ ุชูุซูู ุงูุชุฑุชูุจ ุงููุณุจู ูููููุงุช ุจุดูู ุฃูุถู ูู absolute positionsุ ุฎุตูุตูุง ููุชุณูุณูุงุช ุงูุทูููุฉ.",
                "reference": "Transformer-XL (Dai et al., 2019), T5, relative positional embeddings in Transformers",
                "category": ["mechanisms"],
                "classification": "mechanism",
                "classificationType": "advanced"
            }


            ,

            {
                "id": 21,
                "type": "Sparsemax Attention",
                "description": "ุจุฏูู ูููSoftmax ูููุฏ ุชูุฒูุน attention ูุชูุงุซุฑ ูุญุชูู ุนูู ุฃุตูุงุฑ ุญููููุฉุ ุจุญูุซ ูุฑูุฒ ุงููููุฐุฌ ุนูู ุฃูู ุงูุชูููุงุช ููุท.",
                "rnnProblem": "ูู ุญุงู ุงุณุชุฎุฏุงู RNN attentionุ ุนุงุฏุฉ ููุณุชุฎุฏู Softmax ุงูุฐู ูุนุทู probabilities > 0 ููู ุนูุตุฑุ ุญุชู ุบูุฑ ุงููููุ ููุง ูุฌุนู ุงูุชุฑููุฒ ุฃูู ูุถูุญูุง ููููู interpretability.",
                "detailedSolution": "Sparsemax ูููู ุจุนูู Euclidean projection ุนูู simplex ูุดุงุจู ููSoftmaxุ ูููู ููุชุฌ output ูุชูุงุซุฑ (sparse) ูุญุชูู ุนูู exact zeros. ูุฐุง ูุนูู ุฃู ุงููููุฐุฌ ูุฑูุฒ ููุท ุนูู ุฃูู tokens ููุถุน ุงููattention ุนูู ุงูุจุงูู = 0 ูุนููุงุ ูููุณ ูุฌุฑุฏ ูููุฉ ุตุบูุฑุฉ. โ๏ธ ุงูุญุณุงุจุงุช ุฃุจุทุฃ ูู Softmax ุงูุชูููุฏูุ ููู ูุนุทู interpretability ุฃุนูู ููุณูุญ ุจุชุญุฏูุฏ ุงููููุงุช ุงูุฃูุซุฑ ุฃูููุฉ ุจูุถูุญ.",
                "problem": "ูุญุชุงุฌ ุงุฎุชูุงุฑ ุฃูู ุงููููุงุช ููุท ูู ุงูุชุณูุณู ูุงูุชุฌุงูู ุงูุตุฑูุญ ููุจุงููุ ูุชุญุณูู interpretability ูุงููุนุงููุฉ ุงูุญุณุงุจูุฉ.",
                "reference": "Sparsemax (Martins & Astudillo, 2016), Sparse Attention Mechanisms",
                "category": ["mechanisms"],
                "classification": "mechanism",
                "classificationType": "advanced"
            },





            {
                "id": 22,
                "type": "Grouped Query Attention (GQA)",
                "description": "ูุดุงุฑูุฉ K/V ุจูู ูุฌููุนุงุช ูู ุงููqueriesุ ูุญู ูุณุท ุจูู Multi-Head Attention ุงูุชูููุฏู (MHA) ูMulti-Query Attention (MQA)ุ ูุชูููู ุงูุฐุงูุฑุฉ ูุงูุญุณุงุจุงุช.",
                "rnnProblem": "ูุด ูุดููุฉ ูู RNNุ ุฏู optimization ุฎุงุต ุจุงููMulti-Head Attention ูู Transformers ูุชูููู ุงูุฐุงูุฑุฉ ูุงููcompute.",
                "detailedSolution": "ูู GQAุ ุจุฏู ุฃู ูููู ููู head K,V ูููุตู (MHA) ุฃู ูู ุงููheads ูุดุงุฑููุง ููุณ K,V (MQA)ุ ููุณู ุงููheads ุฅูู ูุฌููุนุงุช ููู ูุฌููุนุฉ ุชุดุงุฑู ููุณ K,V. ูุซุงู: 32 heads ู4 groups โ 8 K/V pairs ุจุฏู 32ุ ูุง ูููุฑ ุญูุงูู 70% ูู ุงูุฐุงูุฑุฉ ูู KV cache ูุน ุงูุญูุงุธ ุนูู ุฌูุฏุฉ ุงูุฃุฏุงุก ูุฑูุจุฉ ุฌุฏูุง ูู MHA. ูุฐุง ูููู ุจุดูู ูุจูุฑ memory footprint ูcompute ูุน ุงูุญูุงุธ ุนูู ุงููุฏุฑุฉ ุนูู parallel processing ููู head group.",
                "problem": "ูุญุชุงุฌ ุชูููู ุงุณุชููุงู ุงูุฐุงูุฑุฉ ูุงูุญุณุงุจุงุช ูู Multi-Head Attention ูุน ุงูุญูุงุธ ุนูู ุฌูุฏุฉ ุงูุฃุฏุงุก.",
                "reference": "GQA (Ainslie et al., 2023), Llama 2, Mistral, Efficient Multi-Head Attention",
                "category": ["mechanisms"],
                "classification": "mechanism",
                "classificationType": "advanced"
            }

            ,


            {
                "id": 23,
                "type": "Cross-Attention with Latents (Perceiver)",
                "description": "ุถุบุท ุงูุชุณูุณู ุงูุทููู ูุนุฏุฏ ุตุบูุฑ ูู latent tokens ุซู ุชุทุจูู attention ุนููููุ ูุชูููู ุงูุชุนููุฏ ุงูุญุณุงุจู ูุน ุงูุญูุงุธ ุนูู ุงููุนูููุงุช ุงููููุฉ.",
                "rnnProblem": "RNN/LSTM ุจูุถุบุท ูู ุงููsequence ูู hidden state ูุงุญุฏ ููุทุ ููุง ูุคุฏู ูููุฏุงู ูููุฉ ูุจูุฑุฉ ูู ุงููุนูููุงุช ุนูุฏ sequences ุทูููุฉ ุฌุฏูุง.",
                "detailedSolution": "ูู Perceiverุ ูุณุชุฎุฏู cross-attention ูุถุบุท sequence ุทููู (n tokens) ุฅูู ุนุฏุฏ ุตุบูุฑ ูู latent tokens (m << n). ุจุนุฏ ุฐููุ ูููู ุจุนูู self-attention ููุท ุนูู ุงููlatentsุ ุซู cross-attention ูุฅุฑุฌุงุน ุงููุนูููุงุช ูููsequence ุงูุฃุตูู ุฅุฐุง ูุฒู ุงูุฃูุฑ. ูุฐุง ูููู ุงูุชุนููุฏ ุงูุญุณุงุจู ูู O(nยฒ) ุฅูู <span style='color:green; display: inline-block; direction: ltr;'>O(nm + mยฒ)</span>. ูุซุงู: n=100,000 ูm=512 โ ุชูููุฑ ูุงุฆู ูู ุงูุญุณุงุจุงุช ูุงูุฐุงูุฑุฉ ูุน ุงูุญูุงุธ ุนูู ุฏูุฉ ุงูุชูุซูู.",
                "problem": "ูุญุชุงุฌ ุถุบุท sequences ุทูููุฉ ุฌุฏูุง ุฅูู ุชูุซูู compact ูefficient ุฏูู ููุฏุงู ุงููุนูููุงุช ุงููููุฉ.",
                "reference": "Perceiver, Perceiver IO (Jaegle et al., 2021), Efficient Attention for Long Sequences",
                "category": ["mechanisms"],
                "classification": "mechanism",
                "classificationType": "advanced"
            }, {
                "id": 24,
                "type": "RoPE (Rotary Position Embedding)",
                "description": "ุชูุซูู ุงูููุถุน ุนุจุฑ ุชุฏููุฑ ุงููQ ูK ูู ุงููุถุงุก ุงููุฑูุจ ุจุญูุซ ุชุนุชูุฏ ุงููattention ุนูู ุงููุณุงูุฉ ุงููุณุจูุฉ ุจุดูู ุทุจูุนู.",
                "rnnProblem": "RNN/LSTM ุนูุฏู positional info ุถูููุฉ ูุฃูู sequentialุ ููููุง ุชุถูุน ูุน ุงูุชุณูุณูุงุช ุงูุทูููุฉุ ุจูููุง learned absolute embeddings ุบูุฑ ูุงุจูุฉ ููุชุนููู (extrapolation) ุฅูู sequences ุฃุทูู ูู ุงูุชุฏุฑูุจ.",
                "detailedSolution": "ูู RoPEุ ูุทุจู rotation matrix ุนูู Q ูK ููููุง ููููุถุน: <span style='color:green; display: block; direction: ltr; text-align: left;'>f(x,m) = xยทe<sup>imฮธ</sup></span> ูู ุงููุถุงุก ุงููุฑูุจ. ูุชูุฌุฉ ุฐููุ ุงููinner product ุจูู Q ูK ูุนูุณ ุงููุณุงูุฉ ุงููุณุจูุฉ ุจูู ุงูุชูููุงุช ุจุดูู ุทุจูุนู. ุงููููุฒุงุช: \n- ูุฏุนู extrapolation ูุชุณูุณูุงุช ุฃุทูู ูู ุงูุชุฏุฑูุจ\n- ูุง ูุญุชุงุฌ parameters ุฅุถุงููุฉ\n- ูุญูู natural decay ูุน ุฒูุงุฏุฉ ุงููุณุงูุฉ ุจูู ุงูุชูููุงุช\n- ูุญุงูุธ ุนูู ูุฏุฑุฉ ุงููููุฐุฌ ุนูู ุงูุชุนุงูู ูุน sequences ุทูููุฉ ุจููุงุกุฉ",
                "problem": "ูุญุชุงุฌ position encoding ูุนูู ูุน sequences ุฃุทูู ูู ุงูุชู ุชู ุชุฏุฑูุจ ุงููููุฐุฌ ุนูููุง ุฏูู ููุฏุงู ุงููุนูููุงุช.",
                "reference": "RoFormer (Su et al., 2021), LLaMA, GPT-NeoX, Rotary Position Embeddings",
                "category": ["mechanisms", "positional"],
                "classification": "mechanism",
                "classificationType": "advanced"
            },

            {
                "id": 25,
                "type": "ALiBi (Attention with Linear Biases)",
                "description": "ุฅุถุงูุฉ bias ุฎุทู ุจุณูุท ุนูู attention scores ุญุณุจ ุงููุณุงูุฉ ุจูู ุงูุชูููุงุชุ ุจุฏูู ุงุณุชุฎุฏุงู position embeddings ุชูููุฏูุฉ.",
                "rnnProblem": "Position embeddings ุงูุชูููุฏูุฉ ุชุถูู parameters ุฌุฏูุฏุฉ ูุตุนุจ extrapolation ูุชุณูุณูุงุช ุฃุทูู ูู ุงูุชุฏุฑูุจุ ุจูููุง RNN ูุฏููุง ูุนูููุงุช ููุถุนูุฉ ุถูููุฉ ููููุง ุชุถูุน ูุน ุงููtime.",
                "detailedSolution": "ูู ALiBiุ ุจุฏู position embeddingsุ ูุถูู bias ุฎุทู ููู pair ูู ุงูุชูููุงุช: <span style='color:green; display: block; direction: ltr; text-align: left;'>score<sub>ij</sub> = q<sub>i</sub>ยทk<sub>j</sub> - mยท|i-j|</span> ุญูุซ m ูู slope ูุฎุชูู ููู head. ูุฐุง ูุชูุญ: \n- ุนุฏู ุงูุญุงุฌุฉ ูุฃู parameters ุฅุถุงููุฉ\n- ุฏุนู extrapolation ููุชุงุฒ ูุชุณูุณูุงุช ุฃุทูู ุจูุซูุฑ ูู ุงูุชุฏุฑูุจ\n- ุงูุญูุงุธ ุนูู simplicity ูุณุฑุนุฉ ุงูุญุณุงุจุงุช\nูุนูู bias ุงูุฎุทู ุนูู ุชูุถูู ุงูุงูุชุจุงู ูููููุงุช ุงูุฃูุฑุจ ูุณุจููุงุ ููุง ูุญุณู generalization ุฏูู ุฒูุงุฏุฉ ุญุฌู ุงููููุฐุฌ.",
                "problem": "ูุญุชุงุฌ position encoding ุจุณูุท ุฌุฏุงูุ ุณุฑูุนุ ููุงุจู ููุชูุณุน ูุชุณูุณูุงุช ุฃุทูู ูู ุงูุชุฏุฑูุจ.",
                "reference": "ALiBi (Press et al., 2022), BLOOM, MPT, Linear Biases for Transformers",
                "category": ["mechanisms", "positional"],
                "classification": "mechanism",
                "classificationType": "advanced"
            },


            {
                "id": 26,
                "type": "Multi-Query Attention (MQA)",
                "description": "ุฌููุน ุงููheads ุชุดุงุฑู ููุณ K ูVุ ููู ุญุงูุฉ ุฎุงุตุฉ ูู GQAุ ูุชูููู ุงุณุชููุงู ุงูุฐุงูุฑุฉ ูุชุณุฑูุน inference.",
                "rnnProblem": "ูุด ูุดููุฉ ูู RNNุ ูุฐุง optimization ุฎุงุต ุจุงููMulti-Head Attention ูุชูููู KV cache memory ุฃุซูุงุก inference.",
                "detailedSolution": "ูู MQAุ ุจุฏู ุฃู ูููู ููู head K,V ูููุตูุ ูู ุงููheads ุชุดุชุฑู ูู single K,V pair ูุงุญุฏ. ูุฐุง ูููุฑ memory ูุจูุฑ ูู KV cache (~95%) ููุณุฑุน inference ุจุดูู ููุญูุธ. ุงูุนูุจ ุฃูู ูุฏ ูููู ููุงู ุชุฑุงุฌุน ุทููู ูู ุงูุฌูุฏุฉ ููุงุฑูุฉ ุจูMHAุ ููู ูููู ุชุนููุถ ุฐูู ุจุฒูุงุฏุฉ dimensionality ูููK (d<sub>k</sub>) ุฅุฐุง ูุฒู ุงูุฃูุฑ. ููุงุณุจ ุฌุฏูุง ููููุงุฐุฌ ุงููุจูุฑุฉ ุนูุฏ ุงูุญุงุฌุฉ ุฅูู decoding ุณุฑูุน ุนูู sequences ุทูููุฉ.",
                "problem": "ูุญุชุงุฌ ุชูููู KV cache memory ุจุดูู ูุจูุฑ ูุชุณุฑูุน inference ุนูู sequences ุทูููุฉ.",
                "reference": "Fast Transformer Decoding (Shazeer, 2019), PaLM, Efficient Transformer Decoding",
                "category": ["mechanisms"],
                "classification": "mechanism",
                "classificationType": "advanced"
            },




            {
                "id": 27,
                "type": "Sliding Window + RoPE (Extended Context)",
                "description": "ุฏูุฌ sliding window attention ูุน RoPE ูููpositional encodingุ ูุฏุนู context ุทููู ุฌุฏูุง ุจููุงุกุฉ ุนุงููุฉ.",
                "rnnProblem": "RNN/LSTM ูุญุฏูุฏ ูู long context memoryุ ูุฃูู ูุนุชูุฏ ุนูู hidden state ูุชุฑุงูู sequentially ููุตุนุจ ุนููู ุงูุงุญุชูุงุธ ุจุณูุงู ุทููู ุฌุฏูุง.",
                "detailedSolution": "ูู ูุฐุง ุงูุฃุณููุจุ ูุณุชุฎุฏู sliding window attention (ูุซูุงู 4096 tokens) ุจุญูุซ ูู token ูุฑูุฒ ููุท ุนูู tokens ูุญููุฉ ุถูู ุงููุงูุฐุฉุ ูุน ุชุทุจูู RoPE ุนูู Q ูK ูุชุถููู positional information ุจุดูู ูููู ุชุนูููู ุนูู ุชุณูุณูุงุช ุฃุทูู. ูููู ุฃูุถูุง ุฅุถุงูุฉ global attention tokens ูุชุบุทูุฉ ุฃูู ุงูุนูุงุตุฑ ุนุจุฑ ูู ุงูุณูุงู. ูุฐุง ูุชูุญ ุงูุชุนุงูู ูุน contexts ุทูููุฉ ุฌุฏูุง (100k+ tokens) ุจููุงุกุฉ ุนุงููุฉุ ูุน ุงูุญูุงุธ ุนูู ููู ูุญูู (local) ูุนุงููู (global).",
                "problem": "ูุญุชุงุฌ context window ุทููู ุฌุฏูุง ูุน ุงูุญูุงุธ ุนูู ุงูููุงุกุฉ ุงูุญุณุงุจูุฉ ูููู local ูglobal patterns.",
                "reference": "Mistral 7B (sliding window), Longformer patterns, Extended Context Transformers",
                "category": ["mechanisms", "sparse"],
                "classification": "mechanism",
                "classificationType": "advanced"
            }




        ];

        // ุงูุจูุงูุงุช: ุงูุชุญุณููุงุช ุงูุชูููุฐูุฉ ูุงููุนูุงุฑูุงุช (5 ุฃููุงุน)
        const attentionOptimizations = [{
            id: 1,
            type: "Flash Attention",
            description: "ุชุญุณูู ุชูููุฐู ูุชูููู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ (IO-aware)",
            classification: "ุชุญุณูู ุชูููุฐู",
            function: "ูุญุณุจ attention ุจูููุงุช ุตุบูุฑุฉุ ุงูุฐุงูุฑุฉ ุฃูู ููุชูุฌุฉ ุตุญูุญุฉ (IO-aware)",
            reference: "FlashAttention (Dao et al., 2022)",
            category: ["optimizations"],
            classificationType: "implementation"
        }, {
            id: 2,
            type: "PyTorch Basic Attention",
            description: "ูุงุฌูุฉ ุจุฑูุฌุฉ ุฃุณุงุณูุฉ ููุงูุชุจุงู ูู PyTorch",
            classification: "ุชูููุฐ ูู ุฅุทุงุฑ ุนูู",
            function: "ูุฏุนู batchingุ maskingุ GPU acceleration",
            reference: "torch.nn.functional.scaled_dot_product_attention",
            category: ["optimizations", "pytorch"],
            classificationType: "pytorch"
        }, {
            id: 3,
            type: "PyTorch Multi-Head Attention",
            description: "ูุงุฌูุฉ ุจุฑูุฌุฉ ููุงูุชุจุงู ูุชุนุฏุฏ ุงูุฑุคูุณ ูู PyTorch",
            classification: "ุชูููุฐ ูู ุฅุทุงุฑ ุนูู",
            function: "multi-head projections + concatenationุ fully GPU optimized",
            reference: "torch.nn.MultiheadAttention",
            category: ["optimizations", "pytorch"],
            classificationType: "pytorch"
        }, {
            id: 4,
            type: "PyTorch Causal Attention",
            description: "ูุงุฌูุฉ ุจุฑูุฌุฉ ููุงูุชุจุงู ุงูุณุจุจู (ูุน ููุงุน) ูู PyTorch",
            classification: "ุชูููุฐ ูู ุฅุทุงุฑ ุนูู",
            function: "masking ูุถูู causality ูุน parallel computation",
            reference: "torch.nn.TransformerDecoder",
            category: ["optimizations", "pytorch"],
            classificationType: "pytorch"
        }, {
            id: 5,
            type: "Mixture of Experts (MoE)",
            description: "ูุนูุงุฑูุฉ ุชูุฌูู ุฏููุงูููู ููุฎุจุฑุงุก",
            classification: "ูุนูุงุฑูุฉ ุญุณุงุจ ุดุฑุทู",
            function: "Queries ุชุฑูุญ ูููexperts ุงูููุงุณุจุฉุ ูู expert ูุนุงูุฌ subset ุจุณ โ ุฃูุซุฑ ููุงุกุฉ",
            reference: "Switch Transformer, MoE in Large Models",
            category: ["optimizations"],
            classificationType: "architecture"
        }];



        // ุชููุฆุฉ ุงูุชุทุจูู
        document.addEventListener('DOMContentLoaded', function() {
            const mechanismsBody = document.getElementById('mechanismsBody');
            const optimizationsBody = document.getElementById('optimizationsBody');
            const searchInput = document.getElementById('searchInput');
            const clearSearchBtn = document.getElementById('clearSearch');
            const filterButtons = document.querySelectorAll('.filter-btn');
            const resetSearchBtn = document.getElementById('resetSearch');
            const noResultsDiv = document.getElementById('noResults');
            const copyLinkBtn = document.getElementById('copyLink');
            const copyNotification = document.getElementById('copyNotification');
            const totalMechanismsSpan = document.getElementById('totalMechanisms');
            const totalOptimizationsSpan = document.getElementById('totalOptimizations');
            const totalFoundSpan = document.getElementById('totalFound');

            let currentFilter = 'all';
            let currentSearch = '';

            // ุชุญููู ุญุงูุฉ ุงูุจุญุซ ูุงูููุชุฑ ูู localStorage
            const savedFilter = localStorage.getItem('attentionFilter') || 'all';
            const savedSearch = localStorage.getItem('attentionSearch') || '';

            // ุชุนููู ุงูููู ุงููุญููุธุฉ
            currentFilter = savedFilter;
            currentSearch = savedSearch;
            searchInput.value = savedSearch;

            // ุชูุนูู ุงูุฒุฑ ุงูููุงุณุจ
            filterButtons.forEach(btn => {
                if (btn.dataset.filter === savedFilter) {
                    btn.classList.add('active');
                } else {
                    btn.classList.remove('active');
                }
            });

            // ุนุฑุถ/ุฅุฎูุงุก ุฒุฑ ูุณุญ ุงูุจุญุซ
            if (savedSearch.trim()) {
                clearSearchBtn.classList.add('visible');
            }

            // ุชุนููู ุงูุฅุญุตุงุฆูุงุช ุงูุฃูููุฉ
            totalMechanismsSpan.textContent = attentionMechanisms.length;
            totalOptimizationsSpan.textContent = attentionOptimizations.length;
            totalFoundSpan.textContent = attentionMechanisms.length + attentionOptimizations.length;

            // ุชุญุฏูุซ ุฃุฑูุงู ุงูุฃุฒุฑุงุฑ
            updateFilterButtonsCount();

            // ุนุฑุถ ุงูุจูุงูุงุช
            renderTables();

            // ูุธููุฉ ุงูุจุญุซ
            searchInput.addEventListener('input', function(e) {
                currentSearch = e.target.value.trim().toLowerCase();
                localStorage.setItem('attentionSearch', currentSearch);

                if (currentSearch) {
                    clearSearchBtn.classList.add('visible');
                } else {
                    clearSearchBtn.classList.remove('visible');
                }

                renderTables();
            });

            // ูุณุญ ุงูุจุญุซ
            clearSearchBtn.addEventListener('click', function() {
                searchInput.value = '';
                currentSearch = '';
                localStorage.setItem('attentionSearch', '');
                clearSearchBtn.classList.remove('visible');
                renderTables();
                searchInput.focus();
            });

            // ุฃุฒุฑุงุฑ ุงูููุชุฑุฉ
            filterButtons.forEach(btn => {
                btn.addEventListener('click', function() {
                    const filter = this.dataset.filter;

                    // ุชุญุฏูุซ ุงูุฃุฒุฑุงุฑ ุงููุดุทุฉ
                    filterButtons.forEach(b => b.classList.remove('active'));
                    this.classList.add('active');

                    currentFilter = filter;
                    localStorage.setItem('attentionFilter', filter);

                    renderTables();
                });
            });

            // ุฅุนุงุฏุฉ ุชุนููู ุงูุจุญุซ
            resetSearchBtn.addEventListener('click', function() {
                searchInput.value = '';
                currentSearch = '';
                localStorage.setItem('attentionSearch', '');
                clearSearchBtn.classList.remove('visible');

                // ุฅุนุงุฏุฉ ุชุนููู ุงูููุชุฑ ูููู
                filterButtons.forEach(b => b.classList.remove('active'));
                filterButtons[0].classList.add('active');
                currentFilter = 'all';
                localStorage.setItem('attentionFilter', 'all');

                renderTables();
            });

            // ูุณุฎ ุงูุฑุงุจุท ูุน ุญุงูุฉ ุงูุจุญุซ
            copyLinkBtn.addEventListener('click', function(e) {
                e.preventDefault();

                // ุฅูุดุงุก ุฑุงุจุท ูุน ูุนููุงุช ุงูุจุญุซ
                const url = new URL(window.location);
                url.searchParams.set('filter', currentFilter);
                url.searchParams.set('search', currentSearch);

                // ูุณุฎ ุงูุฑุงุจุท ููุญุงูุธุฉ
                navigator.clipboard.writeText(url.toString())
                    .then(() => {
                        // ุฅุธูุงุฑ ุฅุดุนุงุฑ ุงููุณุฎ
                        copyNotification.classList.add('show');
                        setTimeout(() => {
                            copyNotification.classList.remove('show');
                        }, 3000);
                    })
                    .catch(err => {
                        console.error('ูุดู ูุณุฎ ุงูุฑุงุจุท: ', err);
                        alert('ูุดู ูุณุฎ ุงูุฑุงุจุท. ูุฑุฌู ุงููุญุงููุฉ ูุฑุฉ ุฃุฎุฑู.');
                    });
            });

            // ุชุญููู ูุนููุงุช URL ุนูุฏ ูุชุญ ุงูุตูุญุฉ
            const urlParams = new URLSearchParams(window.location.search);
            const urlFilter = urlParams.get('filter');
            const urlSearch = urlParams.get('search');

            if (urlFilter) {
                currentFilter = urlFilter;
                localStorage.setItem('attentionFilter', urlFilter);

                filterButtons.forEach(b => {
                    b.classList.remove('active');
                    if (b.dataset.filter === urlFilter) {
                        b.classList.add('active');
                    }
                });
            }

            if (urlSearch !== null) {
                currentSearch = urlSearch;
                searchInput.value = urlSearch;
                localStorage.setItem('attentionSearch', urlSearch);

                if (urlSearch.trim()) {
                    clearSearchBtn.classList.add('visible');
                }
            }

            // ูุธููุฉ ุชุญุฏูุซ ุฃุนุฏุงุฏ ุงูุฃุฒุฑุงุฑ
            function updateFilterButtonsCount() {
                filterButtons.forEach(btn => {
                    const filter = btn.dataset.filter;
                    let count = 0;

                    if (filter === 'all') {
                        count = attentionMechanisms.length + attentionOptimizations.length;
                        btn.textContent = `ุงููู (${count})`;
                    } else if (filter === 'mechanisms') {
                        count = attentionMechanisms.length;
                        btn.textContent = `ุขููุงุช ุงูุงูุชุจุงู (${count})`;
                    } else if (filter === 'optimizations') {
                        count = attentionOptimizations.length;
                        btn.textContent = `ุชุญุณููุงุช ุชูููุฐูุฉ (${count})`;
                    } else if (filter === 'sparse') {
                        count = attentionMechanisms.filter(item =>
                            item.category.includes('sparse')
                        ).length;
                        btn.textContent = `Sparse Attention (${count})`;
                    } else if (filter === 'basic') {
                        count = attentionMechanisms.filter(item =>
                            item.category.includes('basic')
                        ).length;
                        btn.textContent = `ุฃุณุงุณู (${count})`;
                    } else if (filter === 'pytorch') {
                        count = attentionOptimizations.filter(item =>
                            item.category.includes('pytorch')
                        ).length;
                        btn.textContent = `PyTorch (${count})`;
                    }
                });
            }

            // ูุธููุฉ ุนุฑุถ ุงูุฌุฏุงูู
            function renderTables() {
                let filteredMechanisms = attentionMechanisms;
                let filteredOptimizations = attentionOptimizations;

                // ุชุทุจูู ุงูููุชุฑ
                if (currentFilter !== 'all') {

                    filteredMechanisms = attentionMechanisms.filter(item =>
                        item.category.includes(currentFilter)
                    );
                    filteredOptimizations = attentionOptimizations.filter(item =>
                        item.category.includes(currentFilter)
                    );
                }

                // ุชุทุจูู ุงูุจุญุซ
                if (currentSearch) {
                    const searchTerm = currentSearch.toLowerCase();

                    filteredMechanisms = filteredMechanisms.filter(item => {
                        return Object.values(item).some(value => {
                            if (typeof value === 'string') {
                                return value.toLowerCase().includes(searchTerm);
                            } else if (Array.isArray(value)) {
                                return value.some(v => v.toLowerCase().includes(searchTerm));
                            }
                            return false;
                        });
                    });

                    filteredOptimizations = filteredOptimizations.filter(item => {
                        return Object.values(item).some(value => {
                            if (typeof value === 'string') {
                                return value.toLowerCase().includes(searchTerm);
                            } else if (Array.isArray(value)) {
                                return value.some(v => v.toLowerCase().includes(searchTerm));
                            }
                            return false;
                        });
                    });
                }

                // ุชูุฑูุบ ุงูุฌุฏุงูู
                mechanismsBody.innerHTML = '';
                optimizationsBody.innerHTML = '';

                const totalVisible = filteredMechanisms.length + filteredOptimizations.length;

                if (totalVisible === 0) {
                    noResultsDiv.style.display = 'block';
                    totalFoundSpan.textContent = '0';
                } else {
                    noResultsDiv.style.display = 'none';
                    totalFoundSpan.textContent = totalVisible;

                    // ุฅุถุงูุฉ ุงูุตููู ูุฌุฏูู ุงูุขููุงุช
                    filteredMechanisms.forEach(item => {
                        const row = document.createElement('tr');
                        row.classList.add('fade-in');

                        // ุชูููุฒ ุงูุตููู ุงููุงุชุฌุฉ ุนู ุงูุจุญุซ
                        if (currentSearch) {
                            row.classList.add('search-match');
                        }

                        // ุชุญุฏูุฏ ููู ุจุทุงูุฉ ุงูุชุตููู
                        let classificationBadge = '';
                        if (item.classificationType === "basic") {
                            classificationBadge = '<span class="classification-badge classification-basic">ุฃุณุงุณู</span>';
                        } else if (item.classificationType === "sparse") {
                            classificationBadge = '<span class="classification-badge classification-sparse">Sparse</span>';
                        } else if (item.classificationType === "advanced") {
                            classificationBadge = '<span class="classification-badge classification-architecture">ูุชูุฏู</span>';
                        }

                        row.innerHTML = `
        <td class="text-center"><span class="row-number">${item.id}</span></td>
        <td class="text-center"><span class="type-badge">${highlightText(item.type)}</span>${classificationBadge}</td>
        <td>${highlightText(item.description)}</td>
        <td class="rnn-problem-cell">${highlightText(item.rnnProblem)}</td>
        <td class="solution-cell">${highlightText(item.detailedSolution)}</td>
        <td>${highlightText(item.problem)}</td>
        <td><span class="category-label">${highlightText(item.reference)}</span></td>
    `;

                        mechanismsBody.appendChild(row);
                    });
                    // ุฅุถุงูุฉ ุงูุตููู ูุฌุฏูู ุงูุชุญุณููุงุช
                    filteredOptimizations.forEach(item => {
                        const row = document.createElement('tr');
                        row.classList.add('fade-in');

                        // ุชูููุฒ ุงูุตููู ุงููุงุชุฌุฉ ุนู ุงูุจุญุซ
                        if (currentSearch) {
                            row.classList.add('search-match');
                        }

                        // ุชุญุฏูุฏ ููู ุจุทุงูุฉ ุงูุชุตููู
                        let classificationBadge = '';
                        if (item.classificationType === "implementation") {
                            classificationBadge = '<span class="classification-badge classification-implementation">ุชูููุฐ</span>';
                        } else if (item.classificationType === "pytorch") {
                            classificationBadge = '<span class="classification-badge classification-pytorch">PyTorch</span>';
                        } else if (item.classificationType === "architecture") {
                            classificationBadge = '<span class="classification-badge classification-architecture">ูุนูุงุฑูุฉ</span>';
                        }

                        row.innerHTML = `
                            <td class="text-center"><span class="row-number">${((attentionMechanisms.length-1) + item.id)}</span></td>
                            <td class="text-center"><span class="type-badge">${highlightText(item.type)}</span>${classificationBadge}</td>
                            <td><span class="category-label">${highlightText(item.classification)}</span></td>
                            <td>${highlightText(item.function)}</td>
                            <td><span class="category-label">${highlightText(item.reference)}</span></td>
                        `;

                        optimizationsBody.appendChild(row);
                    });
                }
            }

            // ูุธููุฉ ุชูููุฒ ูุต ุงูุจุญุซ
            function highlightText(text) {
                if (!currentSearch || typeof text !== 'string') {
                    return text;
                }

                const searchRegex = new RegExp(`(${currentSearch.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')})`, 'gi');
                return text.replace(searchRegex, '<span class="search-highlight">$1</span>');
            }

            // ุฅุถุงูุฉ ุชุฃุซูุฑ ุงูุชูุฑูุฑ ุงูุณูุณ ูุฃุฒุฑุงุฑ ุงูููุชุฑุฉ
            filterButtons.forEach(btn => {
                btn.addEventListener('click', function() {
                    const firstTableTop = document.querySelector('#mechanismsTable').offsetTop;
                    window.scrollTo({
                        top: firstTableTop - 100,
                        behavior: 'smooth'
                    });
                });
            });

            const popup = document.getElementById('popup');
            const iframe = document.getElementById('popupFrame');
            const openPopupLink = document.getElementById('openPopup');
            const closePopupBtn = document.getElementById('closePopup');

            openPopupLink.addEventListener('click', function(e) {
                e.preventDefault();

                // Add class to body when popup is open
                document.body.classList.add('popup-open');

                // Show popup
                popup.style.display = 'flex';


                iframe.src = '/transExP'; // Good demo of attention
            });

            closePopupBtn.addEventListener('click', function() {
                // Hide popup
                popup.style.display = 'none';

                // Remove class from body
                document.body.classList.remove('popup-open');

                // Clear iframe source to stop loading
                iframe.src = 'https://transexp-4kl3a7t74-abdullahashraf12s-projects.vercel.app/';
            });

            // Close popup when clicking outside the container
            popup.addEventListener('click', function(e) {
                if (e.target === popup) {
                    popup.style.display = 'none';
                    document.body.classList.remove('popup-open');
                    iframe.src = '';
                }
            });

            // Close popup with Escape key
            document.addEventListener('keydown', function(e) {
                if (e.key === 'Escape' && popup.style.display === 'flex') {
                    popup.style.display = 'none';
                    document.body.classList.remove('popup-open');
                    iframe.src = '';
                }
            });

            // Handle iframe loading state
            iframe.addEventListener('load', function() {
                console.log('Iframe loaded successfully');
            });

            iframe.addEventListener('error', function() {
                console.error('Failed to load iframe content');
                // You could show an error message here if needed
            });

        });
    </script>
</body>

</html>